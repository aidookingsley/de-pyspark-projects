{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c758b511",
   "metadata": {},
   "source": [
    "Employee Salary Analysis with PySpark\n",
    "\n",
    "Problem Statement:\n",
    "This script processes employee salary data to:\n",
    "1. Filter employees earning above a specified salary threshold ($60,000)\n",
    "2. Count how many high-earning employees exist in each department\n",
    "3. Provide insights into departmental salary distributions\n",
    "\n",
    "Input Data Structure:\n",
    "- Department: String (Engineering, HR, Marketing, Sales)\n",
    "- Salary: Integer (annual salary amounts)\n",
    "\n",
    "Key Operations:\n",
    "1. Creates Spark DataFrame from sample data\n",
    "2. Applies salary filter using column expression\n",
    "3. Aggregates results by department\n",
    "4. Displays department-wise counts of high earners\n",
    "\n",
    "Business Applications:\n",
    "- Compensation analysis\n",
    "- Departmental budget planning\n",
    "- Pay equity assessments\n",
    "- Talent retention strategies\n",
    "\n",
    "Technical Details:\n",
    "- Uses PySpark DataFrame API\n",
    "- Demonstrates filtering and grouping operations\n",
    "- Runs on local Spark session (scalable to cluster)\n",
    "\n",
    "# Note: In production, you would:\n",
    "# 1. Read from actual data source (CSV, database, etc.)\n",
    "# 2. Parameterize the salary threshold\n",
    "# 3. Add error handling and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Filter Employees by Salary and Count by Department\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "\n",
    "    (\"Engineering\", 70000),\n",
    "\n",
    "    (\"Engineering\", 80000),\n",
    "\n",
    "    (\"HR\", 50000),\n",
    "\n",
    "    (\"HR\", 55000),\n",
    "\n",
    "    (\"Marketing\", 60000),\n",
    "\n",
    "    (\"Marketing\", 65000),\n",
    "\n",
    "    (\"Sales\", 40000)\n",
    "\n",
    "]\n",
    "columns = [\"Department\", \"Salary\"]\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define the salary threshold\n",
    "salary_threshold = 60000\n",
    "\n",
    "# Filter employees with salary greater than the threshold\n",
    "filtered_df = df.filter(col(\"Salary\") > salary_threshold)\n",
    "# Count the number of employees in each department\n",
    "count_by_department = filtered_df.groupBy(\"Department\").count()\n",
    "\n",
    "# Show the result\n",
    "count_by_department.show()\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sptfy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
